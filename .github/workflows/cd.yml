name: Connectivity Microservice CD

on:
  push:
    branches: [main]

permissions:
  contents: read

env:
  TF_BACKEND_BUCKET: ${{ secrets.TF_BACKEND_BUCKET }}
  TF_BACKEND_KEY: connectivity/infra/terraform/aws/terraform.tfstate
  TF_BACKEND_REGION: ${{ secrets.AWS_REGION }}
  TF_BACKEND_DDB_TABLE: ${{ secrets.TF_BACKEND_DDB_TABLE }}

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKERHUB_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ secrets.DOCKERHUB_USERNAME }}/connectivity-microservice
          tags: |
            type=sha,prefix=main-
            type=raw,value=latest

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Image digest
        run: echo ${{ steps.meta.outputs.digest }}

  infra-apply:
    name: Provision Microservice AWS Resources (Terraform) - Phase 1
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init (infra - remote state)
        working-directory: infra/terraform/aws
        run: |
          terraform init -input=false -upgrade \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"

      - name: Terraform Validate (infra)
        working-directory: infra/terraform/aws
        run: terraform validate

      - name: Clean up scheduled-for-deletion secrets
        run: |
          echo "Checking for secrets scheduled for deletion..."
          aws secretsmanager list-secrets --region ${{ secrets.AWS_REGION }} \
            --filters Key=name,Values=connectivity --output json | \
            jq -r '.SecretList[] | select(.DeletedDate != null) | .ARN' | \
            while read -r secret_arn; do
              if [ -n "$secret_arn" ]; then
                echo "Force deleting secret: $secret_arn"
                aws secretsmanager delete-secret --secret-id "$secret_arn" \
                  --region ${{ secrets.AWS_REGION }} \
                  --force-delete-without-recovery || true
              fi
            done
          echo "Waiting 30 seconds for AWS to process..."
          sleep 30

      - name: Clean up old IAM policies
        run: |
          echo "Checking for old connectivity IAM policies with timestamps..."
          aws iam list-policies --scope Local --query 'Policies[?contains(PolicyName, `connectivity-production-external-secrets-policy-`) || contains(PolicyName, `connectivity-production-policy-`)]' --output json | \
            jq -r '.[] | select(.PolicyName | test("connectivity-production-(external-secrets-)?policy-[0-9]+")) | .Arn' | \
            while read -r policy_arn; do
              if [ -n "$policy_arn" ]; then
                echo "Detaching and deleting old policy: $policy_arn"
                
                # Detach from all roles first
                ATTACHED_ROLES=$(aws iam list-entities-for-policy --policy-arn "$policy_arn" --query 'PolicyRoles[].RoleName' --output text)
                for role in $ATTACHED_ROLES; do
                  echo "  Detaching from role: $role"
                  aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn" || true
                done
                
                # Delete all non-default versions
                VERSIONS=$(aws iam list-policy-versions --policy-arn "$policy_arn" --query 'Versions[?!IsDefaultVersion].VersionId' --output text)
                for version in $VERSIONS; do
                  echo "  Deleting version: $version"
                  aws iam delete-policy-version --policy-arn "$policy_arn" --version-id "$version" || true
                done
                
                # Delete the policy
                echo "  Deleting policy: $policy_arn"
                aws iam delete-policy --policy-arn "$policy_arn" || true
              fi
            done
          echo "Old policies cleaned up"
          
          echo ""
          echo "Checking for policies with fixed names that need cleanup..."
          # Also delete the fixed-name policies if they exist (we'll recreate them)
          for policy_name in "connectivity-production-service-policy" "connectivity-production-external-secrets-policy"; do
            POLICY_ARN=$(aws iam list-policies --scope Local --query "Policies[?PolicyName=='$policy_name'].Arn" --output text)
            if [ -n "$POLICY_ARN" ]; then
              echo "Found existing policy: $policy_name ($POLICY_ARN)"
              echo "  Detaching from all roles..."
              ATTACHED_ROLES=$(aws iam list-entities-for-policy --policy-arn "$POLICY_ARN" --query 'PolicyRoles[].RoleName' --output text)
              for role in $ATTACHED_ROLES; do
                echo "    Detaching from role: $role"
                aws iam detach-role-policy --role-name "$role" --policy-arn "$POLICY_ARN" || true
              done
              
              echo "  Deleting all non-default versions..."
              VERSIONS=$(aws iam list-policy-versions --policy-arn "$POLICY_ARN" --query 'Versions[?!IsDefaultVersion].VersionId' --output text)
              for version in $VERSIONS; do
                echo "    Deleting version: $version"
                aws iam delete-policy-version --policy-arn "$POLICY_ARN" --version-id "$version" || true
              done
              
              echo "  Deleting policy..."
              aws iam delete-policy --policy-arn "$POLICY_ARN" || true
              echo "  âœ“ Policy deleted"
            fi
          done
          echo "âœ“ All old policies cleaned up"

      - name: Import existing resources (if any)
        working-directory: infra/terraform/aws
        continue-on-error: true
        run: |
          echo "Attempting to import existing resources..."
          
          # Import security group (need to get the ID first)
          SG_ID=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=connectivity-production-rds-sg" "Name=vpc-id,Values=vpc-011c7f75f98629972" \
            --query 'SecurityGroups[0].GroupId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            echo "Found security group: $SG_ID"
            terraform import -input=false \
              -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
              -var "aws_region=${{ secrets.AWS_REGION }}" \
              aws_security_group.rds "$SG_ID" || echo "Security group already imported"
          else
            echo "Security group not found, will be created"
          fi
          
          # Import DB subnet group
          terraform import -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            aws_db_subnet_group.rds connectivity-production-rds-subnets || echo "DB subnet group not found or already imported"
          
          # Import DB parameter group
          terraform import -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            aws_db_parameter_group.rds connectivity-production-rds-params || echo "DB parameter group not found or already imported"
          
          # Import CloudWatch log group
          terraform import -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            aws_cloudwatch_log_group.connectivity /aws/eks/production/connectivity-microservice || echo "Log group not found or already imported"
          
          # Import IAM role
          terraform import -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            'module.irsa.aws_iam_role.this[0]' connectivity-production-service-irsa || echo "IAM role not found or already imported"
          
          # Import security group rules (need SG ID)
          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            # Get existing ingress rules
            RULES=$(aws ec2 describe-security-group-rules \
              --filters "Name=group-id,Values=$SG_ID" \
              --query 'SecurityGroupRules[?IsEgress==`false`].SecurityGroupRuleId' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$RULES" ]; then
              RULE_ARRAY=($RULES)
              # Import first rule (CIDR)
              if [ ${#RULE_ARRAY[@]} -gt 0 ]; then
                terraform import -input=false \
                  -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
                  -var "aws_region=${{ secrets.AWS_REGION }}" \
                  aws_vpc_security_group_ingress_rule.rds_ingress_cidr "${RULE_ARRAY[0]}" || echo "Rule 1 already imported"
              fi
              # Import second rule (from nodes)
              if [ ${#RULE_ARRAY[@]} -gt 1 ]; then
                terraform import -input=false \
                  -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
                  -var "aws_region=${{ secrets.AWS_REGION }}" \
                  'aws_vpc_security_group_ingress_rule.rds_ingress_from_nodes[0]' "${RULE_ARRAY[1]}" || echo "Rule 2 already imported"
              fi
            fi
          fi
          
          # Import RDS instance
          terraform import -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            aws_db_instance.postgres connectivity-production-pg || echo "RDS instance not found or already imported"
          
          # Import Secrets Manager secrets (need ARNs, not names)
          echo "Importing Secrets Manager secrets..."
          
          # Get ARN for app secret
          APP_SECRET_ARN=$(aws secretsmanager describe-secret \
            --secret-id connectivity-production/application-5c2c \
            --region ${{ secrets.AWS_REGION }} \
            --query 'ARN' --output text 2>/dev/null || echo "")
          if [ -n "$APP_SECRET_ARN" ]; then
            terraform import -input=false \
              -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
              -var "aws_region=${{ secrets.AWS_REGION }}" \
              aws_secretsmanager_secret.app "$APP_SECRET_ARN" || echo "App secret already imported"
          fi
          
          # Get ARN for RDS secret
          RDS_SECRET_ARN=$(aws secretsmanager describe-secret \
            --secret-id connectivity-production/rds/postgresql-5c2c \
            --region ${{ secrets.AWS_REGION }} \
            --query 'ARN' --output text 2>/dev/null || echo "")
          if [ -n "$RDS_SECRET_ARN" ]; then
            terraform import -input=false \
              -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
              -var "aws_region=${{ secrets.AWS_REGION }}" \
              aws_secretsmanager_secret.rds_credentials "$RDS_SECRET_ARN" || echo "RDS secret already imported"
          fi
          
          # Get ARN for connections secret
          CONN_SECRET_ARN=$(aws secretsmanager describe-secret \
            --secret-id connectivity-production/connections-5c2c \
            --region ${{ secrets.AWS_REGION }} \
            --query 'ARN' --output text 2>/dev/null || echo "")
          if [ -n "$CONN_SECRET_ARN" ]; then
            terraform import -input=false \
              -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
              -var "aws_region=${{ secrets.AWS_REGION }}" \
              aws_secretsmanager_secret.app_connections "$CONN_SECRET_ARN" || echo "Connections secret already imported"
          fi
          
          # Import random_password (it exists in state from previous run)
          terraform import -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}" \
            random_password.db_password none 2>/dev/null || echo "random_password will be created"
          
          # Import egress rule if it exists
          if [ -n "$SG_ID" ] && [ "$SG_ID" != "None" ]; then
            EGRESS_RULES=$(aws ec2 describe-security-group-rules \
              --filters "Name=group-id,Values=$SG_ID" \
              --query 'SecurityGroupRules[?IsEgress==`true`].SecurityGroupRuleId' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$EGRESS_RULES" ]; then
              EGRESS_ARRAY=($EGRESS_RULES)
              if [ ${#EGRESS_ARRAY[@]} -gt 0 ]; then
                terraform import -input=false \
                  -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
                  -var "aws_region=${{ secrets.AWS_REGION }}" \
                  aws_vpc_security_group_egress_rule.rds_egress "${EGRESS_ARRAY[0]}" || echo "Egress rule already imported"
              fi
            fi
          fi

      - name: Terraform Apply (all microservice resources)
        working-directory: infra/terraform/aws
        run: |
          echo "Applying all Terraform resources..."
          terraform apply -auto-approve -input=false \
            -var "tf_backend_bucket=$TF_BACKEND_BUCKET" \
            -var "aws_region=${{ secrets.AWS_REGION }}"
          
          echo ""
          echo "âœ“ Terraform apply completed"
          echo ""
          echo "Verifying state was saved to S3..."
          echo "Backend: s3://$TF_BACKEND_BUCKET/$TF_BACKEND_KEY"
          
          echo ""
          echo "Outputs after apply:"
          terraform output
          
          echo ""
          echo "Verifying state file exists in S3..."
          echo "Checking: s3://$TF_BACKEND_BUCKET/$TF_BACKEND_KEY"
          
          if aws s3api head-object --bucket "$TF_BACKEND_BUCKET" --key "$TF_BACKEND_KEY" 2>/dev/null; then
            echo "âœ“ State file confirmed in S3"
            STATE_INFO=$(aws s3api head-object --bucket "$TF_BACKEND_BUCKET" --key "$TF_BACKEND_KEY")
            STATE_SIZE=$(echo "$STATE_INFO" | jq -r '.ContentLength')
            echo "  Size: $STATE_SIZE bytes"
            echo "  Last Modified: $(echo "$STATE_INFO" | jq -r '.LastModified')"
          else
            echo "âš ï¸ WARNING: State file not found in S3!"
            echo "Checking if backend is configured correctly..."
            terraform show -json | jq -r '.values.root_module.resources[0].address' || echo "No resources in state"
          fi

  app-deploy:
    name: Deploy Application to EKS (kubectl/kustomize) - Phase 2
    runs-on: ubuntu-latest
    needs: [build-and-push, infra-apply]
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }}

      - name: Install kubectl
        run: |
          KUBECTL_VERSION=$(curl -sL https://dl.k8s.io/release/stable.txt)
          curl -LO "https://dl.k8s.io/release/${KUBECTL_VERSION}/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_wrapper: false

      - name: Terraform Init and Load State
        working-directory: infra/terraform/aws
        run: |
          echo "Checking Terraform configuration files..."
          ls -la *.tf
          echo ""
          echo "Checking backend configuration..."
          grep -A 5 'backend "s3"' backend.tf || echo "Backend block not found!"
          
          echo ""
          echo "Initializing Terraform to read outputs..."
          echo "Backend config:"
          echo "  Bucket: $TF_BACKEND_BUCKET"
          echo "  Key: $TF_BACKEND_KEY"
          echo "  Region: $TF_BACKEND_REGION"
          echo "  DynamoDB: $TF_BACKEND_DDB_TABLE"
          
          terraform init -input=false -reconfigure \
            -backend-config="bucket=$TF_BACKEND_BUCKET" \
            -backend-config="key=$TF_BACKEND_KEY" \
            -backend-config="region=$TF_BACKEND_REGION" \
            -backend-config="dynamodb_table=$TF_BACKEND_DDB_TABLE" \
            -backend-config="encrypt=true"
          
          echo ""
          echo "Waiting for S3 state to sync (eventual consistency)..."
          sleep 15
          
          echo ""
          echo "Verifying backend configuration..."
          terraform version
          
          echo ""
          echo "Pulling state from S3 backend..."
          STATE_CONTENT=$(terraform state pull)
          if [ -z "$STATE_CONTENT" ] || [ "$STATE_CONTENT" = "{}" ]; then
            echo "âš ï¸ WARNING: State file is empty or not found in S3!"
            echo "Backend: s3://$TF_BACKEND_BUCKET/$TF_BACKEND_KEY"
            echo "This might mean the infra-apply job hasn't completed yet."
          else
            echo "âœ“ State file loaded successfully from S3"
            echo "State size: $(echo "$STATE_CONTENT" | wc -c) bytes"
          fi
          
          echo ""
          echo "All available outputs:"
          terraform output
          
          echo ""
          echo "Checking critical outputs individually..."
          echo -n "  rds_endpoint: "
          terraform output -raw rds_endpoint 2>/dev/null && echo "âœ“" || echo "MISSING"
          echo -n "  rds_secret_arn: "
          terraform output -raw rds_secret_arn 2>/dev/null && echo "âœ“" || echo "MISSING"
          echo -n "  irsa_role_arn: "
          terraform output -raw irsa_role_arn 2>/dev/null && echo "âœ“" || echo "MISSING"

      - name: Update kubeconfig
        run: |
          echo "Getting cluster name from Terraform output..."
          CLUSTER_NAME=$(terraform -chdir=infra/terraform/aws output -raw cluster_name)
          
          echo "Cluster name: $CLUSTER_NAME"
          echo "Updating kubeconfig..."
          aws eks update-kubeconfig --name "$CLUSTER_NAME" --region ${{ secrets.AWS_REGION }}

      - name: Check if deployment is ready
        id: check_ready
        env:
          TF_DIR: infra/terraform/aws
        run: |
          echo "Checking if all required Terraform outputs are available..."
          
          # Try to get each output, suppress errors
          SECRET_NAME=$(terraform -chdir=$TF_DIR output -raw app_secret_name 2>/dev/null || echo "")
          RABBIT_URL=$(terraform -chdir=$TF_DIR output -raw rabbitmq_amqp_url 2>/dev/null || echo "")
          RDS_ENDPOINT=$(terraform -chdir=$TF_DIR output -raw rds_endpoint 2>/dev/null || echo "")
          RDS_SECRET_ARN=$(terraform -chdir=$TF_DIR output -raw rds_secret_arn 2>/dev/null || echo "")
          IRSA_ROLE=$(terraform -chdir=$TF_DIR output -raw irsa_role_arn 2>/dev/null || echo "")
          
          # Check if any are missing
          if [ -z "$SECRET_NAME" ] || [ -z "$RABBIT_URL" ] || [ -z "$RDS_ENDPOINT" ] || [ -z "$RDS_SECRET_ARN" ] || [ -z "$IRSA_ROLE" ]; then
            echo "âŒ Some Terraform outputs are not available yet."
            echo "This is normal on first deployment while RDS is being created."
            echo ""
            echo "Output Status:"
            echo "  SECRET_NAME: ${SECRET_NAME:-MISSING}"
            echo "  RABBIT_URL: ${RABBIT_URL:-MISSING}"
            echo "  RDS_ENDPOINT: ${RDS_ENDPOINT:-MISSING}"
            echo "  RDS_SECRET_ARN: ${RDS_SECRET_ARN:-MISSING}"
            echo "  IRSA_ROLE: ${IRSA_ROLE:-MISSING}"
            echo ""
            echo "deployment_ready=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ Infrastructure not fully ready. Skipping deployment steps."
            echo "ðŸ’¡ Run the pipeline again in a few minutes after RDS creation completes."
          else
            echo "âœ… All required outputs are available!"
            echo ""
            echo "Output Status:"
            echo "  SECRET_NAME: âœ“"
            echo "  RABBIT_URL: âœ“"
            echo "  RDS_ENDPOINT: âœ“"
            echo "  RDS_SECRET_ARN: âœ“"
            echo "  IRSA_ROLE: âœ“"
            echo ""
            echo "deployment_ready=true" >> $GITHUB_OUTPUT
            echo "ðŸš€ Proceeding with deployment..."
          fi

      - name: Prepare AWS Secrets for app
        if: steps.check_ready.outputs.deployment_ready == 'true'
        env:
          TF_DIR: infra/terraform/aws
        run: |
          echo "Getting Terraform outputs..."
          SECRET_NAME=$(terraform -chdir=$TF_DIR output -raw app_secret_name)
          RABBIT_URL=$(terraform -chdir=$TF_DIR output -raw rabbitmq_amqp_url)
          RDS_ENDPOINT=$(terraform -chdir=$TF_DIR output -raw rds_endpoint)
          
          echo "Getting RDS credentials from Secrets Manager..."
          RDS_SECRET_ARN=$(terraform -chdir=$TF_DIR output -raw rds_secret_arn)
          RDS_CREDS=$(aws secretsmanager get-secret-value --secret-id "$RDS_SECRET_ARN" --query SecretString --output text)
          DB_USER=$(echo "$RDS_CREDS" | jq -r '.username')
          DB_PASSWORD=$(echo "$RDS_CREDS" | jq -r '.password')
          DB_NAME=$(echo "$RDS_CREDS" | jq -r '.dbname')
          
          echo "Validating credentials..."
          echo "  DB_USER: $DB_USER"
          echo "  DB_NAME: $DB_NAME"
          echo "  RDS_ENDPOINT: $RDS_ENDPOINT"
          echo "  DB_PASSWORD length: ${#DB_PASSWORD}"
          
          if [ -z "$DB_PASSWORD" ] || [ "$DB_PASSWORD" = "null" ]; then
            echo "âŒ ERROR: DB_PASSWORD is empty or null!"
            exit 1
          fi
          
          echo "Building DATABASE_URL with SSL..."
          DATABASE_URL="postgresql://${DB_USER}:${DB_PASSWORD}@${RDS_ENDPOINT}:5432/${DB_NAME}?sslmode=require"
          
          echo "Updating application secret in Secrets Manager..."
          aws secretsmanager put-secret-value \
            --secret-id "$SECRET_NAME" \
            --secret-string "$(jq -n \
              --arg secret_key "${{ secrets.DJANGO_SECRET_KEY }}" \
              --arg debug "False" \
              --arg allowed_hosts "${{ secrets.ALLOWED_HOSTS || '*' }}" \
              --arg db_url "$DATABASE_URL" \
              --arg rabbit_host "$(echo $RABBIT_URL | cut -d'@' -f2 | cut -d':' -f1)" \
              --arg rabbit_port "5672" \
              --arg rabbit_user "${{ secrets.RABBITMQ_USER || 'guest' }}" \
              --arg rabbit_pass "${{ secrets.RABBITMQ_PASSWORD || 'guest' }}" \
              --arg rabbit_vhost "${{ secrets.RABBITMQ_VHOST || '/' }}" \
              --arg rabbit_exchange "${{ secrets.RABBITMQ_EXCHANGE || 'connectivity' }}" \
              --arg doc_auth_queue "${{ secrets.RABBITMQ_DOCUMENT_AUTH_QUEUE || 'document.authentication.requested' }}" \
              --arg doc_auth_key "${{ secrets.RABBITMQ_DOCUMENT_AUTH_ROUTING_KEY || 'document.authentication.requested' }}" \
              --arg user_reg_queue "${{ secrets.RABBITMQ_AUTH_USER_REGISTERED_QUEUE || 'auth.user.registered' }}" \
              --arg user_reg_key "${{ secrets.RABBITMQ_AUTH_USER_REGISTERED_ROUTING_KEY || 'auth.user.registered' }}" \
              --arg ext_api_url "${{ secrets.EXTERNAL_GOVCARPETA_API_URL }}" \
              --arg ext_api_key "${{ secrets.EXTERNAL_GOVCARPETA_API_KEY || '' }}" \
              --arg ext_api_timeout "${{ secrets.EXTERNAL_API_TIMEOUT || '30' }}" \
              --arg jwt_secret "${{ secrets.AUTH_SERVICE_JWT_SECRET }}" \
              --arg jwt_algo "${{ secrets.AUTH_SERVICE_JWT_ALGORITHM || 'HS256' }}" \
              --arg cors_origins "${{ secrets.CORS_ALLOWED_ORIGINS || '' }}" \
              --arg log_level "${{ secrets.LOG_LEVEL || 'INFO' }}" \
              --arg app_env "${{ secrets.APP_ENV || 'production' }}" \
              '{
                SECRET_KEY: $secret_key,
                DEBUG: $debug,
                ALLOWED_HOSTS: $allowed_hosts,
                DATABASE_URL: $db_url,
                RABBITMQ_HOST: $rabbit_host,
                RABBITMQ_PORT: $rabbit_port,
                RABBITMQ_USER: $rabbit_user,
                RABBITMQ_PASSWORD: $rabbit_pass,
                RABBITMQ_VHOST: $rabbit_vhost,
                RABBITMQ_EXCHANGE: $rabbit_exchange,
                RABBITMQ_DOCUMENT_AUTH_QUEUE: $doc_auth_queue,
                RABBITMQ_DOCUMENT_AUTH_ROUTING_KEY: $doc_auth_key,
                RABBITMQ_AUTH_USER_REGISTERED_QUEUE: $user_reg_queue,
                RABBITMQ_AUTH_USER_REGISTERED_ROUTING_KEY: $user_reg_key,
                EXTERNAL_GOVCARPETA_API_URL: $ext_api_url,
                EXTERNAL_GOVCARPETA_API_KEY: $ext_api_key,
                EXTERNAL_API_TIMEOUT: $ext_api_timeout,
                AUTH_SERVICE_JWT_SECRET: $jwt_secret,
                AUTH_SERVICE_JWT_ALGORITHM: $jwt_algo,
                CORS_ALLOWED_ORIGINS: $cors_origins,
                LOG_LEVEL: $log_level,
                APP_ENV: $app_env
              }')" || true

      - name: Render K8s templates
        if: steps.check_ready.outputs.deployment_ready == 'true'
        env:
          TF_DIR: infra/terraform/aws
        run: |
          set -euo pipefail
          SECRET_NAME=$(terraform -chdir=$TF_DIR output -raw app_secret_name)
          REGION='${{ secrets.AWS_REGION }}'
          IRSA_ROLE=$(terraform -chdir=$TF_DIR output -raw irsa_role_arn)
          
          sed -i "s|__AWS_REGION__|$REGION|g" k8s/overlays/prod/externalsecrets.yaml
          sed -i "s|__AWS_SECRET_NAME__|$SECRET_NAME|g" k8s/overlays/prod/externalsecrets.yaml
          sed -i "s|IRSA_ROLE_ARN|$IRSA_ROLE|g" k8s/base/service-account.yaml

      - name: Delete old ExternalSecret
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: |
          kubectl -n connectivity delete externalsecret connectivity-secrets --ignore-not-found=true
          sleep 5

      - name: Deploy application
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: kubectl apply -k k8s/overlays/prod/

      - name: Force rolling update
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: kubectl rollout restart deployment/connectivity-service -n connectivity

      - name: Wait for External Secrets
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: |
          for i in {1..12}; do
            if kubectl -n connectivity get secret connectivity-secrets &>/dev/null; then
              echo "Secret created successfully!"
              break
            fi
            echo "Waiting... (attempt $i/12)"
            sleep 5
          done

      - name: Wait for deployment rollout
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: kubectl -n connectivity rollout status deploy/connectivity-service --timeout=300s

      - name: Verify migrations ran successfully
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: |
          echo "Checking if migrations ran successfully via init container..."
          POD=$(kubectl -n connectivity get pods -l app=connectivity-service --field-selector=status.phase=Running -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
          if [ -n "$POD" ]; then
            echo "âœ“ Pod is running: $POD"
            echo ""
            echo "=== Init Container Logs (Migrations) ==="
            kubectl -n connectivity logs "$POD" -c run-migrations || echo "Init container logs not available (already completed)"
          else
            echo "Warning: No running pods found yet"
          fi

      - name: Display Access Information
        if: steps.check_ready.outputs.deployment_ready == 'true'
        run: |
          echo "=== Deployment Summary ==="
          ALB_HOSTNAME=$(kubectl -n connectivity get ingress connectivity-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          if [ -n "$ALB_HOSTNAME" ]; then
            echo "  Hostname: $ALB_HOSTNAME"
            echo "  API: http://$ALB_HOSTNAME/api/connectivity"
            echo "  Health: http://$ALB_HOSTNAME/api/connectivity/health/"
            echo "  Docs: http://$ALB_HOSTNAME/api/connectivity/docs/"
          else
            echo "  Pending... Waiting for ALB"
          fi
